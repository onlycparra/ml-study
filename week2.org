#+TITLE: Machine Learning -- Week 2
#+AUTHOR: Claudio Parra
#+OPTIONS: toc:nil
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css" />
#+PROPERTY: header-args :eval no-export
[[file:week1.org][Week 1]] [[file:week3.org][Week 3]]

* Multivariate Linear Regression
** Notation for Multiple Features
- The set of training examples to be used is usually in the shape of a table where there are many *feature* columns and one *target* column (the price of apartments, in this example).

  |    size | bedrooms |     age |  price |
  |---------+----------+---------+--------|
  |    1200 |        3 |       5 |   9600 |
  |    1500 |        4 |       3 |  12000 |
  |     960 |        8 |       2 |   8000 |

  Each training example in the table (each row) is said to be a *data point* \(d\). The \(i\)-th datapoint is denoted by \(d^{(i)}\). The array of features \(x\) are used by our *hypothesis* \(h_{\Theta}(x)\) to predict the target \(y\).

  \[\text{\(i\)-th row data point}\rightarrow
  \quad
  d^{(i)}=\begin{bmatrix}
  x^{(i)} \\
  y^{(i)} \\
  \end{bmatrix}
  \quad
  \begin{split}
  &\leftarrow \text{features of \(i\)-th row} \\
  &\leftarrow \text{target of \(i\)-th row} \\
  \end{split}\]

  then, the whole dataset \(D\) can be represented as an array of datapoints:

  \[D = \begin{bmatrix}
  d^{(1)}, d^{(2)}, d^{(2)}, d^{(3)}, \dots
  \end{bmatrix}\]

- *Extended list of features* of \(d^{(i)}\): For convenience, it is added an extra 1 at the beginning of the vector \(x\) so in the hypothesis function we have \(n\) parameters and \(n\) features.
  This 1 matches with the *bias parameter* \(\Theta_0\).
  \[x^{(i)} =
  \begin{bmatrix}
  x^{(i)}_0 = 1 \\
  x^{(i)}_1 \\
  x^{(i)}_2 \\
  x^{(i)}_3 \\
  x^{(i)}_4 \\
  \end{bmatrix}\]

- Then, the hypothesis ends up just being the multiplication of the transposed vector of parameters \(\Theta^T\),  with the extended vector of features \(x\). (Remember that \(x_0=1\).)
  \[\begin{split}
  h_{\Theta}(x) &\xrightarrow{\text{predicts}} y \\
  h_{\Theta}(x) &= \Theta_0x_0 + \Theta_1x_1 + \Theta_2x_2 + \Theta_3x_3 + \cdots \\
  h_{\Theta}(x) &= \begin{bmatrix}\Theta_0, \Theta_1, \Theta_2 \cdots\end{bmatrix} \times
                   \begin{bmatrix}x_0\\ x_1\\ x_2\\ \cdots\end{bmatrix} \\
  h_{\Theta}(x) &= \Theta^T \times x \\
  \end{split}\]
** Gradient Descent (GD) for Multiple Variables
- The idea is the same as with less variables. Given that the Cost function is \(\Theta^{T}x^{(i)}\), where \(x^{(i)}\) is the \(i\)-th training example, and \(x^{(i)}_{0} = 1\), to match \(\Theta_0\), then all the derivatives to update all the \(\Theta_{j}\) are easy to compute:
  \[\begin{split}
  \Theta^{\text{new}}_j := \Theta^{\text{old}}_j -
  \textcolor{red}{\alpha}
  \textcolor{green}{\frac{\delta}{\delta\Theta_j} J(\Theta)}&\\
  %%
  \textcolor{green}{\frac{\delta}{\delta\Theta_j} J(\Theta)}& =
  \frac{1}{m}\sum^{m}_{i=1}\left(
  \big(
  h_{\Theta}(x^{(i)})-y^{(i)}
  \big)
  x^{(i)}_{j}\right)\\
  \end{split}\]
  - Green: Partial derivative of \(J\) with respect to \(\Theta_j\)
  - Red: Learning rate
** GD: Feature Scaling and Normalization
*** Feature Scaling
- Make sure the values of different features take values that range in similar values, this makes GD converge quickly. (this makes me thing on the disproportioned representation of skin surface in the brain, hands and face are huge, legs and backs are tiny.)
- Different people have different takes in what is a sensible range, but the take-away is "make sure they are in a kinda similar range".
- This can be acheived by dividing every instance of a feature \(x_j\) by the max value seen in this feature in the whole dataset. This scales all values \(x_j\)s to the range \([0-1]\).
*** Mean Normalization
- Making the values gravitate around 0, by replacing \(x_j\) with \(x_j - \mu_j\)
- This does not apply to \(x_0\), because it needs to be \(1\).
*** Both together
- The vid. talks about the denominator being either the range of values \((\text{max}(x_j)-\text{min}(x_j))\) or std deviation. Doing this you end up with values roughly between \([-0.5,0.5]\).
  \[x^{\text{norm.}}_j := \frac{x_j - \mu_{j}}{\text{max}(x_j) - \text{min}(x_j)}\]

- This denominator was not mentioned in the videos, but I think it is pretty much the same as before, the "max", by which I am scaling, is also normalized. This leads to values in the range \([-1,1]\).
  \[x^{\text{norm.}}_j := \frac{x_j - \mu_{j}}{\text{max}(x_j) \textcolor{blue}{- \mu_{j}}}\]

** GD: Learning Rate \(\alpha\)
- The point of learning is to find a \(\Theta^{\star}\) such that \(J(\Theta^{\star})\) is small [fn:1], meaning that the hypothesis \(h_{\Theta^{\star}}(x)\) is, in average, not doing bad for any \(x\) you throw at it.
- Supposedly, the more iterations of learning we accumulate (X axis in the right plot) we get smaller and smaller \(J(\Theta)\), which is the discrepancy between \(h_{\Theta}(x)\) and \(y\). In other words, we should be getting closer and closer to our \(\Theta^{\star}\).
- Then, the point of \(\alpha\) is to determine how quickly we go in the direction of the gradient of \(J(\Theta)\). If my changes in \(\Theta\) (X axis of the left plot) are too abrupt, then, even if I go in the correct direction of the gradient (towards a valley), I can anyway overshoot (as seen by the red arrows in the left plot).
- The effect is that as the training cummulates more and more iterations, we don't necessarily approach to a smaller \(J(\Theta)\) (as seen in the right plot).
- A good debugging technique is to plot \(J(\Theta\) (the right plot) as we refine \(\Theta\) with each iteration, so we can see whether it converges or not.
  #+attr_html: :style width: min(600px,100%);
  [[file:week2/learning_rate_a.png]]
- Here an example with a smaller \(\alpha\). The steps are smaller, so it should take longer to to converge, but the chances of actually converging to our \(\Theta^{\star}\) without getting lost in the way, are better.
  #+attr_html: :style width: min(600px,100%);
  [[file:week2/learning_rate_b.png]]
- An automatic convergence test could be "if \(J(\Theta)\) is smaller than some \(\varepsilon\), then we say that \(J\) has converged". But it is often difficult to establish the actual value of such \(\varepsilon\).

- It has been demonstrated (by some ML nerds) that if \(\alpha\) is sufficiently small, then it is guaranteed that \(J(\Theta)\) *will decrease at every single iteration*. The cost of that is that the learning happens to be super slow. Here three examples that shows the feeling of what happens when we vary \(\alpha\):
  - A \(\alpha=0.1\)
  - B \(\alpha=0.01\)
  - C \(\alpha=1\)
  #+attr_html: :style width: min(600px,100%);
  [[file:week2/learning_rate_diff_alpha.png]]
** Features and Polynomial Regression
- Our hypothesis need not be linear (as shown below).
  \[h_{\Theta}(x) =
  \Theta_0 +
  \Theta_1x_1 +
  \Theta_2x_2 +
  \Theta_3x_3 + \cdots\]
- Sometimes we may think a quadratic or cubic hypothesis fits better the data.
  \[h_{\Theta}(x) =
  \Theta_0 +
  \Theta_1\textcolor{red}{x} +
  \Theta_2\textcolor{red}{x^2} +
  \Theta_3\textcolor{red}{x^3} + \cdots\]
- If we have only one original feature \(x_1\), we can create artificial features and then the rest of the algorithm is the same.
  \[x_2 = x^2_1;\quad x_2 = x^3_1\]
- Or compose new features based on serveral originals. For example, if in a database of houses we have:
  - frontage length.
  - depth lenhth.
  We may create a new feature *area* \(=\) frontage \(\times\) depth. And now my hypothesis may be in function of the area, rather than linear lengths.

* Computing Parameters Automatically
:PROPERTIES:
:header-args+: :exports both :results output
:END:
** Normal Equation
- Gradient descent is an iterative algorithm. An analytical approach is to solve for \(\Theta\).
- For example, take this dataset
  #+attr_html: :style width: min(600px,100%);
  [[file:week2/normal_fun_1.png]]
- Then by computing \(\Theta = (X^TX)^{-1}X^Ty\), we get the best \(\Theta\).
  #+begin_src octave
  X = [ 1 2104 5 1 45;
        1 1416 3 2 40;
        1 1534 3 2 30;
        1  852 2 1 36];
  y = [460;
       232;
       315;
       178];
  theta = pinv(X'*X)*X'*y
  %
  % test it
  x_1 = [1 2104 5 1 45];
  x_1 * theta
  #+end_src

  #+RESULTS:
  : theta =
  : 
  :    188.4003
  :      0.3866
  :    -56.1382
  :    -92.9673
  :     -3.7378
  : ans = 460.00

** Gradient Descent vs Normal Equation
| Gradient Descent                                                                      | Normal Equation                                                                                     |
|---------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------|
| Need to choose \(\alpha\)                                                             | No \(\alpha\) to choose                                                                             |
| Iterative approach \(O(kn^2)\)                                                        | No iteration needed. This is an analytical approach \(O(n^3)\)                                      |
| Works well even for huge number of n=features                                         | Need to compute the inverse of a \(n\times n\) matrix. Therefore, slow for large number of features |

 At \(n > 1,000,000\) it is probably the only option in a reasonably modern computer.
** Non-invertible matrix
- Some matrices are not invertible
  - Redundant features: reduce features, or use 'regularization' (seen later).
  - \(m < n\): more features than training points.
- Ocave's ~pinv()~ is *pseudo-inverse*, which does "the right thing" even if the matrix has no inverse (???)
* Programming Assignment
:PROPERTIES:
:header-args+: :exports both :results output
:END:
** Octave Tutorial
*** Basic Operations
Not equal is ~~=~.
#+begin_src octave
1 ~= 2
#+end_src
#+RESULTS:
: ans = 1

Logical operations AND and OR are like in C. but xor is a function.
#+begin_src octave
1 && 3 || xor(7,0)
#+end_src
#+RESULTS:
: ans = 1

Vector/matrix assignments use spaces for elements in the same row, and semicolons for going to the next row.
#+begin_src octave
A=[1 2;
   3 4]
B=[100;
   200
   300]
C=[10 20 30; 40 50 60]
#+end_src
#+RESULTS:
#+begin_example
A =

   1   2
   3   4

B =

   100
   200
   300

C =

   10   20   30
   40   50   60

#+end_example

Finishing the line with semicolon doesn't show it in the output.
#+begin_src octave
a = 32
b = 43;
#+end_src
#+RESULTS:
: a = 32

Show stuff on screen.
#+begin_src octave
a = 37;
disp(a)
printf('Two decimal points: %.2f', a);
#+end_src
#+RESULTS:
: 37
: Two decimal points: 37.00

Auto fill a vector ~start:step:end~ (inclusive). Transpose is the apostrophe.
#+begin_src octave
D = 5:2:13
E = (0:0.2:1)'
#+end_src
#+RESULTS:
#+begin_example
D =

    5    7    9   11   13

E =

        0
   0.2000
   0.4000
   0.6000
   0.8000
   1.0000

#+end_example

*** Plotting Data
#+begin_src octave :results none
% random variable with mean=-10, and variance=10. and plot its histogram with 30 bins
X = -10 + sqrt(10)*randn(1,10000);
hist(X, 30);
% config output figure
set(gcf, 'paperposition', [0 0 8 3], 'visible', 'off')
print('week2/normal_plot.png', '-dpng', '-r300');
#+end_src
#+RESULTS:

#+attr_html: :style width: min(800px,100%);
[[file:week2/normal_plot.png]]

*** Slicing Matrices
#+begin_src octave :session slicing
A = [11 12 13 14 15 16;
     21 22 23 24 25 26;
     31 32 33 34 35 36;
     41 42 43 44 45 46]
% get element in row 2 and column 4
one_elem = A(2,4)
#+end_src
#+RESULTS:
: A =
:
:    11   12   13   14   15   16
:    21   22   23   24   25   26
:    31   32   33   34   35   36
:    41   42   43   44   45   46
: one_elem = 24

To select a range of elements we can use the ~m:n~ notation. You can replace ~n~ with ~end~. This allow us to extract submatrices, columns and rows.
- ~m:n~   :: "everything from m to n" (including elements ~m~ and ~n~)
- ~1:n~   :: "all from the beginning to n"
- ~m:end~ :: "all from m to the end"
- ~1:end~ :: "all from beginning to end". You can also use the shorthand ~:~

Get a single element
#+begin_src octave :session slicing
elem = A(4,3)
#+end_src
#+RESULTS:
: elem = 43

Get a submatrix defining row range (from 2 to 4) and column range (from 3 to 5).
#+begin_src octave :session slicing
sub_matrix = A(2:4,3:5)
#+end_src
#+RESULTS:
: sub_matrix =
:
:    23   24   25
:    33   34   35
:    43   44   45

Get the lower right submatrix.
#+begin_src octave :session slicing
sub_matrix_lower_right = A(2:end,4:end)
#+end_src
#+RESULTS:
: sub_matrix_lower_right =
:
:    24   25   26
:    34   35   36
:    44   45   46

Get the 3rd column.
#+begin_src octave :session slicing
col = A(:,3)
#+end_src
#+RESULTS:
: col =
:
:    13
:    23
:    33
:    43

Get the 4th row.
#+begin_src octave :session slicing
row = A(4,:)
#+end_src
#+RESULTS:
: row =
:
:    41   42   43   44   45   46

*** Moving Data Around
Show current directory.
#+begin_src octave
pwd
#+end_src
#+RESULTS:
: ans = /home/claudio/repos/machine-learning

Content of current directory.
#+begin_src octave
ls
#+end_src
#+RESULTS:
: style.css  week1  week1.html  week1.org  week2	week2.html  week2.org

Content of directory ~week2/~ (in one column ~-1~).
#+begin_src octave
ls -1 week2
#+end_src
#+RESULTS:
: featuresX.dat
: learning_rate_a.png
: learning_rate_b.png
: learning_rate_diff_alpha.png
: normal_fun_1.png
: normal_plot.png

I have a file ~week2/featuresX.dat~.
#+begin_src bash
cat week2/my_dataset.dat
#+end_src
#+RESULTS:
: 2104 5 1 45 460
: 1416 3 2 40 232
: 1534 3 2 30 315
:  852 2 1 36 178

To load it in Octave, use the function ~load~.
#+begin_src octave
data = load('week2/my_dataset.dat')
size(data)
X = data(:,1:(end-1))
Y = data(:,end)
#+end_src
#+RESULTS:
#+begin_example
data =

   2104      5      1     45    460
   1416      3      2     40    232
   1534      3      2     30    315
    852      2      1     36    178

ans =

   4   5

X =

   2104      5      1     45
   1416      3      2     40
   1534      3      2     30
    852      2      1     36

Y =

   460
   232
   315
   178

#+end_example
*** Show and Save Variables
#+begin_src octave :session vars
a = 3;
b = [4 5 6];
C = [1 2; 3 4];
who
whos
#+end_src
#+RESULTS:
#+begin_example
Variables visible from the current scope:

C    a    ans  b
Variables visible from the current scope:

variables in scope: top scope

  Attr   Name        Size                     Bytes  Class
  ====   ====        ====                     =====  =====
         C           2x2                         32  double
         a           1x1                          8  double
         ans         1x13                        13  char
         b           1x3                         24  double

Total is 21 elements using 77 bytes
#+end_example

Save variable to a file (in this case, the matrix ~A~ defined in the previous section)
#+begin_src octave :session slicing
save week2/matrixA.dat A
#+end_src
#+RESULTS:

#+begin_src bash
cat week2/matrixA.dat
#+end_src

#+RESULTS:
#+begin_example
# Created by Octave 7.3.0, Wed Jul 19 19:17:13 2023 PDT <claudio@ncc91277>
# name: A
# type: matrix
# rows: 4
# columns: 6
 11 12 13 14 15 16
 21 22 23 24 25 26
 31 32 33 34 35 36
 41 42 43 44 45 46


#+end_example

[fn:1] \(J(\Theta^{\star})\) small, not necessarily the smallest. That would be to find the global minima (wich is cool, but usually unrealistic).
