# #+LATEX_CLASS: article
# #+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}

#+TITLE: Machine Learning -- Week 2
#+AUTHOR: Claudio Parra
#+OPTIONS: toc:nil
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css" />

[[file:week1.org][Week 1]] [[file:week3.org][Week 3]]

* Multivariate Linear Regression
** Notation for Multiple Features
- The set of training examples to be used is usually in the shape of a table where there are many *feature* columns and one *target* column (the price of apartments, in this example).

  |    size | bedrooms |     age |  price |
  |---------+----------+---------+--------|
  |    1200 |        3 |       5 |   9600 |
  |    1500 |        4 |       3 |  12000 |
  |     960 |        8 |       2 |   8000 |

  Each training example in the table (each row) is said to be a *data point* \(d\). The \(i\)-th datapoint is denoted by \(d^{(i)}\). The array of features \(x\) are used by our *hypothesis* \(h_{\Theta}(x)\) to predict the target \(y\).

  \[\text{\(i\)-th row data point}\rightarrow
  \quad
  d^{(i)}=\begin{bmatrix}
  x^{(i)} \\
  y^{(i)} \\
  \end{bmatrix}
  \quad
  \begin{split}
  &\leftarrow \text{features of \(i\)-th row} \\
  &\leftarrow \text{target of \(i\)-th row} \\
  \end{split}\]

  then, the whole dataset \(D\) can be represented as an array of datapoints:

  \[D = \begin{bmatrix}
  d^{(1)}, d^{(2)}, d^{(2)}, d^{(3)}, \dots
  \end{bmatrix}\]

- *Extended list of features* of \(d^{(i)}\): For convenience, it is added an extra 1 at the beginning of the vector \(x\) so in the hypothesis function we have \(n\) parameters and \(n\) features.
  This 1 matches with the *bias parameter* \(\Theta_0\).
  \[x^{(i)} =
  \begin{bmatrix}
  x^{(i)}_0 = 1 \\
  x^{(i)}_1 \\
  x^{(i)}_2 \\
  x^{(i)}_3 \\
  x^{(i)}_4 \\
  \end{bmatrix}\]

- Then, the hypothesis ends up just being the multiplication of the transposed vector of parameters \(\Theta^T\),  with the extended vector of features \(x\). (Remember that \(x_0=1\).)
  \[\begin{split}
  h_{\Theta}(x) &\xrightarrow{\text{predicts}} y \\
  h_{\Theta}(x) &= \Theta_0x_0 + \Theta_1x_1 + \Theta_2x_2 + \Theta_3x_3 + \cdots \\
  h_{\Theta}(x) &= \begin{bmatrix}\Theta_0, \Theta_1, \Theta_2 \cdots\end{bmatrix} \times
                   \begin{bmatrix}x_0\\ x_1\\ x_2\\ \cdots\end{bmatrix} \\
  h_{\Theta}(x) &= \Theta^T \times x \\
  \end{split}\]
** Gradient Descent (GD) for Multiple Variables
- The idea is the same as with less variables. Given that the Cost function is \(\Theta^{T}x^{(i)}\), where \(x^{(i)}\) is the \(i\)-th training example, and \(x^{(i)}_{0} = 1\), to match \(\Theta_0\), then all the derivatives to update all the \(\Theta_{j}\) are easy to compute:
  \[\begin{split}
  \Theta^{\text{new}}_j := \Theta^{\text{old}}_j -
  \textcolor{red}{\alpha}
  \textcolor{green}{\frac{\delta}{\delta\Theta_j} J(\Theta)}&\\
  %%
  \textcolor{green}{\frac{\delta}{\delta\Theta_j} J(\Theta)}& =
  \frac{1}{m}\sum^{m}_{i=1}\left(
  \big(
  h_{\Theta}(x^{(i)})-y^{(i)}
  \big)
  x^{(i)}_{j}\right)\\
  \end{split}\]
  - Green: Partial derivative of \(J\) with respect to \(\Theta_j\)
  - Red: Learning rate
** GD: Feature Scaling and Normalization
*** Feature Scaling
- Make sure the values of different features take values that range in similar values, this makes GD converge quickly. (this makes me thing on the disproportioned representation of skin surface in the brain, hands and face are huge, legs and backs are tiny.)
- Different people have different takes in what is a sensible range, but the take-away is "make sure they are in a kinda similar range".
- This can be acheived by dividing every instance of a feature \(x_j\) by the max value seen in this feature in the whole dataset. This scales all values \(x_j\)s to the range \([0-1]\).
*** Mean Normalization
- Making the values gravitate around 0, by replacing \(x_j\) with \(x_j - \mu_j\)
- This does not apply to \(x_0\), because it needs to be \(1\).
*** Both together
- The vid. talks about the denominator being either the range of values \((\text{max}(x_j)-\text{min}(x_j))\) or std deviation. Doing this you end up with values roughly between \([-0.5,0.5]\).
  \[x^{\text{norm.}}_j := \frac{x_j - \mu_{j}}{\text{max}(x_j) - \text{min}(x_j)}\]

- This denominator was not mentioned in the videos, but I think it is pretty much the same as before, the "max", by which I am scaling, is also normalized. This leads to values in the range \([-1,1]\).
  \[x^{\text{norm.}}_j := \frac{x_j - \mu_{j}}{\text{max}(x_j) \textcolor{blue}{- \mu_{j}}}\]

** GD: Learning Rate \(\alpha\)
- The point of learning is to find a \(\Theta^{\star}\) such that \(J(\Theta^{\star})\) is small [fn:1], meaning that the hypothesis \(h_{\Theta^{\star}}(x)\) is, in average, not doing bad for any \(x\) you throw at it.
- Supposedly, the more iterations of learning we accumulate (X axis in the right plot) we get smaller and smaller \(J(\Theta)\), which is the discrepancy between \(h_{\Theta}(x)\) and \(y\). In other words, we should be getting closer and closer to our \(\Theta^{\star}\).
- Then, the point of \(\alpha\) is to determine how quickly we go in the direction of the gradient of \(J(\Theta)\). If my changes in \(\Theta\) (X axis of the left plot) are too abrupt, then, even if I go in the correct direction of the gradient (towards a valley), I can anyway overshoot (as seen by the red arrows in the left plot).
- The effect is that as the training cummulates more and more iterations, we don't necessarily approach to a smaller \(J(\Theta)\) (as seen in the right plot).
- A good debugging technique is to plot \(J(\Theta\) (the right plot) as we refine \(\Theta\) with each iteration, so we can see whether it converges or not.
  #+attr_html: :width 600px
  [[file:figs/learning_rate_a.png]]
- Here an example with a smaller \(\alpha\). The steps are smaller, so it should take longer to to converge, but the chances of actually converging to our \(\Theta^{\star}\) without getting lost in the way, are better.
  #+attr_html: :width 600px
  [[file:figs/learning_rate_b.png]]
- An automatic convergence test could be "if \(J(\Theta)\) is smaller than some \(\varepsilon\), then we say that \(J\) has converged". But it is often difficult to establish the actual value of such \(\varepsilon\).

- It has been demonstrated (by some ML nerds) that if \(\alpha\) is sufficiently small, then it is guaranteed that \(J(\Theta)\) *will decrease at every single iteration*. The cost of that is that the learning happens to be super slow. Here three examples that shows the feeling of what happens when we vary \(\alpha\):
  - A \(\alpha=0.1\)
  - B \(\alpha=0.01\)
  - C \(\alpha=1\)
  #+attr_html: :width 600px
  [[file:figs/learning_rate_diff_alpha.png]]
** Features and Polynomial Regression
- Our hypothesis need not be linear (as shown below).
  \[h_{\Theta}(x) =
  \Theta_0 +
  \Theta_1x_1 +
  \Theta_2x_2 +
  \Theta_3x_3 + \cdots\]
- Sometimes we may think a quadratic or cubic hypothesis fits better the data.
  \[h_{\Theta}(x) =
  \Theta_0 +
  \Theta_1\textcolor{red}{x} +
  \Theta_2\textcolor{red}{x^2} +
  \Theta_3\textcolor{red}{x^3} + \cdots\]
- If we have only one original feature \(x_1\), we can create artificial features and then the rest of the algorithm is the same.
  \[x_2 = x^2_1;\quad x_2 = x^3_1\]
- Or compose new features based on serveral originals. For example, if in a database of houses we have:
  - frontage length.
  - depth lenhth.
  We may create a new feature *area* \(=\) frontage \(\times\) depth. And now my hypothesis may be in function of the area, rather than linear lengths.

* Computing Parameters Automatically
:PROPERTIES:
:header-args: :session comp_parms_auto :exports both :results output
:END:
** Normal Equation
- Gradient descent is an iterative algorithm. An analytical approach is to solve for \(\Theta\).
- For example, take this dataset
  #+attr_html: :width 600px
  [[file:figs/normal_fun_1.png]]
- Then by computing \(\Theta = (X^TX)^{-1}X^Ty\), we get the best \(\Theta\).
  #+begin_src octave
  X = [ 1 2104 5 1 45;
        1 1416 3 2 40;
        1 1534 3 2 30;
        1  852 2 1 36];
  y = [460;
       232;
       315;
       178];
  theta = pinv(X'*X)*X'*y
  %
  % test it
  x_1 = [1 2104 5 1 45];
  x_1 * theta
  #+end_src

  #+RESULTS:
  : theta =
  :
  :    188.4003
  :      0.3866
  :    -56.1382
  :    -92.9673
  :     -3.7378
  : ans = 460.00

** Gradient Descent vs Normal Equation
| Gradient Descent                                                                      | Normal Equation                                                                                     |
|---------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------|
| Need to choose \(\alpha\)                                                             | No \(\alpha\) to choose                                                                             |
| Iterative approach \(O(kn^2)\)                                                        | No iteration needed. This is an analytical approach \(O(n^3)\)                                      |
| Works well even for huge number of n=features                                         | Need to compute the inverse of a \(n\times n\) matrix. Therefore, slow for large number of features |

 At \(n > 1,000,000\) it is probably the only option in a reasonably modern computer.
** Non-invertible matrix
- Some matrices are not invertible
  - Redundant features: reduce features, or use 'regularization' (seen later).
  - \(m < n\): more features than training points.
- Ocave's ~pinv()~ is *pseudo-inverse*, which does "the right thing" even if the matrix has no inverse (???)
* Programming Assignment
:PROPERTIES:
:header-args: :session prog_assignment :exports both :results output
:END:

** Octave Tutorial
#+begin_src octave
% not equal is ~=
1 ~= 2
%
% and and or are like in C. but xor is a function
1 && 3 || xor(7,0)
%
% vector/matrix assignments use spaces for elements in the
% same row, and semicolons for going to the next row
A=[1 2;
   3 4]
B=[100;
   200
   300]
C=[10 20 30; 40 50 60]
%
% finishing the line with semicolon doesn't show it in the output.
a = 32
b = 43;
%
% show stuff on screen
disp(a)
%
% auto fill a vector start:step:end (inclusive)
D = 5:2:13
%
% transpose is the apostrophe
E = (0:0.2:1)'
%
% Useful matrices
F = ones(2,3)
G = zeros(2,2)
Hu = rand(1,4) % uniform distribution in [0,1]
Hn = randn(1,4) % normal distribution with mu=0 std=1
I = eye(3)
#+end_src

#+RESULTS:
#+begin_example
ans = 1
ans = 1
A =

   1   2
   3   4
B =

   100
   200
   300
C =

   10   20   30
   40   50   60
a = 32
32
D =

    5    7    9   11   13
E =

        0
   0.2000
   0.4000
   0.6000
   0.8000
   1.0000
F =

   1   1   1
   1   1   1
G =

   0   0
   0   0
Hu =

   0.603433   0.404621   0.016212   0.812026
Hn =

   0.1804   0.3696   0.1758   0.3108
I =

Diagonal Matrix

   1   0   0
   0   1   0
   0   0   1
#+end_example


#+begin_src octave :results none
% random variable with mean=-10, and variance=10
X = -10 + sqrt(10)*randn(1,10000);
% plot its histogram with 50 bins
hist(X, 50);
% set
%set (gca, 'looseinset', [0 0 0 0]);
set(gcf, 'Units', 'pixels', 'Position', [0 0 800 380], 'Visible', 'off')
print('figs/normal_plot.png', '-dpng', '-r300');
#+end_src

#+attr_html: :width 700px
[[file:figs/normal_plot.png]]

[fn:1] \(J(\Theta^{\star})\) small, not necessarily the smallest. That would be to find the global minima (wich is cool, but usually unrealistic).
