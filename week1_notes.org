# #+LATEX_CLASS: article
# #+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}

#+TITLE: Machine Learning -- Week 1
#+AUTHOR: Claudio Parra
#+OPTIONS: toc:nil
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css" />

[[file:week1_notes.org][.]] [[file:week2_notes.org][Week 2]]

* Introduction
- Supervised: you give the right answer to the machine, so it can learn from it.
  - Regression: reconstruct a continuous function.
  - Classification: given a set of labels, apply one (or more?) labels to each input.
- Unsupervised: the machine is asked to find structure within the given data.
** TODO Questions
- [X] classification seems to be a discretization of regression.
  - *ANS* Not quite. It seems that there is no ordering relation between the labels.
- [-] Is there unsupervised learning that is not "searching for labels".
  - *ANS* It looks like
* Model and Cost Function
** Vocabulary (on a supervised learning setup):
- *Dataset*: set of data points.
- *Training set*: dataset used to train the network.
- *i-th Data Point*: the pair \(x^{(i)},y^{(i)}\). When we are in a learning scenario, this is a *training example*
  - *Features* of the \(i^{\text{th}}\) data point: \(x^{(i)}\), input variable or array of input variables.
  - *Target* of the \(i^{\text{th}}\) data point: \(y^{(i)}\), output variable.
  - *Hypothesis*: function (conventionally denoted by \(h\)) that the ML algorithm proposes to be the correct mapping from features to target.
- \(X\): space of input variables.
- \(Y\): space of output variables.
** Model representation
The function \(h\) has the following shape:

\begin{equation}
\begin{split}
h_{\Theta}(x) &= \Theta_0 + \Theta_1x_1 + \Theta_2x_2 \cdots \Theta_{n-1}x_{n-1}\\
              &= [\Theta] \cdot [1|x]
\end{split}
\end{equation}

Where \(x_j\) is the j-th feature of a given data point. The idea is that for a new, unseen \(x'\), \(h_{\Theta}(x') \approx f(x')\) where \(f\) is the actual underlying function producing the dataset.

[[file:figs/ml-diagram.png]]

** Cost/Objective Function
- By convension denoted by \(J\).
- Its parameters are \(\Theta_i\).
- By minimizing \(J\), we are finding parameters \(\Theta_i\) such that \(h_{\Theta}(x^{(i)})\) is close to \(y^{(i)}\).
  \[J(\Theta_0,\Theta_1,\cdots) = \frac{1}{2m}\Sigma^{m}_{i=1}\big(h_{\Theta}(x^{(i)})-y^{(i)}\big)^2\]
  Where \(m\) is the number of training examples in the training set.
- Called *Squared error function* or *error function* or *mean square error*: pretty common for regression problems.
- *Do not confuse* \(h_\Theta(x)\) (the predictor), with \(J(\Theta)\) (the "how good the predictor is")
*** DONE Questions
- [X] I get the \(\frac{1}{m}\) part of the cost function: we want an average /squared-difference/ but why are we didiving by \(2\)? Wouldn't it make more sense to compute the square root?
  - *ANS* The mean is halved \((\frac{1}{2})\) as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the \((\frac{1}{2})\) term.

* Parameter Learning
** Gradient Descent
- The way to update each parameter is this
 \[\Theta_j \leftarrow \Theta_j - \alpha\frac{\delta}{\delta\Theta_j}J(\Theta_0,\Theta_1,\cdots)\]
 Where \(\alpha\) is the /learning rate/. and the expression at the right of \(\alpha\) is the partial derivative of \(J\) with respect to \(\Theta_j\).
- Perform a simultaneous update of all the \(Theta_j\) using the same \(J()\) function in each iteration.
- The smaller \(\alpha\) is, the slower the convergence (smaller steps towards the minimum of \(J\)). The greater \(\alpha\) is, the faster we approach to it. Note that too big may overshoot.
- It is still possible to learn with a fixed \(\alpha\). As \(J\) approaches to its minimum, its derivative approaches to zero. This means that each update to \(\Theta\) gets smaller and smaller. This is good, coz as we get closer to the minimum, we want to be more cautious with the steps, so we don't overshoot.
* Linear Algebra Review
:PROPERTIES:
:header-args: :session model_and_cost :exports both :results output
:END:
- *Matrix*: rows \(\times\) cols. Usually in uppercase.
- *Vector*: a single column. Usually in lowercase.
** Dimensions
#+begin_src octave
% The ; inside of the matrix denotes we are
% going back to a new row.
A = [1, 2, 3; 4, 5, 6; 7, 8, 9; 10, 11, 12]
[rows,cols] = size(A);
rows,cols
A_23 = A(2,3)
%
% Initialize a vector
v = [1;2;3]
[rows,cols] = size(v)
rows,cols
v_3  = v(3)
#+end_src

#+RESULTS:
#+begin_example
A =

    1    2    3
    4    5    6
    7    8    9
   10   11   12
rows = 4
cols = 3
A_23 = 6
v =

   1
   2
   3
rows = 3
cols = 1
rows = 3
cols = 1
v_3 = 3
#+end_example


** Scalar Operations
#+begin_src octave
% Initialize matrix A and B, and scalar s
A = [1, 2, 4; 5, 3, 2]
B = [1, 3, 4; 1, 1, 1]
s = 2
add_AB = A + B
sub_AB = A - B
mult_As = A * s
div_As = A / s
add_As = A + s
#+end_src

#+RESULTS:
#+begin_example
A =

   1   2   4
   5   3   2
B =

   1   3   4
   1   1   1
s = 2
add_AB =

   2   5   8
   6   4   3
sub_AB =

   0  -1   0
   4   2   1
mult_As =

    2    4    8
   10    6    4
div_As =

   0.5000   1.0000   2.0000
   2.5000   1.5000   1.0000
add_As =

   3   4   6
   7   5   4
#+end_example

** Matrix Vector Multiplitation
- I find it easier to see the operation if I dispose the operands using this layout:
  #+begin_example
                 B=
              |  x
        nxn   |  y
      square  |  z
      --------+----
      1  2  3 | [*]  <---- 1x + 2y + 3z
  A=  4  5  6 | [ ]
      7  8  9 | [ ]
      A  B  C | [ ]
  #+end_example

  #+begin_src octave
  % Initialize matrix A and vector v
  A = [1, 2, 3; 4, 5, 6;7, 8, 9]
  v = [1; 2; 1]
  Av = A * v
  #+end_src

  #+RESULTS:
  #+begin_example
  A =

     1   2   3
     4   5   6
     7   8   9
  v =

     1
     2
     1
  Av =

      8
     20
     32
  #+end_example

#+RESULTS:
** Matrix Matrix Multiplication
- Not commutative \(A\times B \neq B\times A\).
- Associative \((A \times B) \times C = A \times (B \times C)\).
- Similar layout idea
  #+begin_example
                    B=
              |  r  u  x
        nxn   |  s  v  y
      square  |  t  w  z
      --------+----------
      1  2  3 | [ ][*][ ]  <---- 1u + 2v + 3w
  A=  4  5  6 | [ ][ ][ ]
      7  8  9 | [ ][ ][ ]
      A  B  C | [ ][ ][ ]
  #+end_example
** Identity
- \(I \times A = A\times I = A\)
  - Note that in general those two are \(I\) with different dimensions.
- \(I\)'s dimension is normally implicit. But sometimes it is denoted as \(I_{n\times n}\) or \(I_{n}\).
** Inverse and Transpose
*** Inverse
- If \(A\) is a square matrix of size \(m \times m\), and it has an inverse, then
  \[A \times A^{-1} = A^{-1} \times A = I\]
  #+begin_src octave
  A = [3 4; 2 16]
  %
  Ainv_numeric = pinv(A)
  A * Ainv_numeric
  % If you pay attention, the product generates tiny rounding errors.
  %
  Ainv_exact = [0.4 -0.1; -0.05 0.075]
  A * Ainv_exact
  % This is the precise answer.
  #+end_src

  #+RESULTS:
  #+begin_example
  A =

      3    4
      2   16
  Ainv_numeric =

     0.400000  -0.100000
    -0.050000   0.075000
  ans =

     1.0000e+00   1.1102e-16
    -2.2204e-16   1.0000e+00
  Ainv_exact =

     0.400000  -0.100000
    -0.050000   0.075000
  ans =

     1.0000  -0.0000
          0   1.0000
  #+end_example

- A matrix without inverse is said to be *Singular* or *Degenerate*
*** Transpose
- The transpose of a matrix \(A\) is denoted by \(A^{T}\).
- In octave, it is experssed with an apostrophe.
  #+begin_src octave
  A = [1 2 3 4; 5 6 7 8]
  At = A'
  #+end_src

  #+RESULTS:
  #+begin_example
  A =

     1   2   3   4
     5   6   7   8
  At =

     1   5
     2   6
     3   7
     4   8
  #+end_example


* COMMENT CODE EXAMPLES
#+begin_src python
import matplotlib.pyplot as plt
x=[1,2,3,4,5]
y=[2,3,2,2,1]
plt.plot(x,y)
plt.savefig('figs/plot.png')
#+end_src

#+RESULTS:
[[file:figs/plot.png]]
